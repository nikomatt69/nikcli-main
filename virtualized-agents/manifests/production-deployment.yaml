apiVersion: v1
kind: ConfigMap
metadata:
  name: virtual-agent-config
  namespace: production
  labels:
    app: virtual-agent
    tier: backend
    version: v1.0.0 # Semantic versioning for tracking
annotations:
  description: "Configuration for virtualized agents in production"
data:
  # Agent-specific settings (key-value pairs for env vars)
  LOG_LEVEL: "info" # Production logging: info to balance verbosity and performance
  AGENT_MODE: "production" # Ensures agents run in optimized mode
  MAX_CONCURRENT_TASKS: "10" # Scalability: Limits per-agent load to prevent overload
  # Integration hook: Env var for custom webhook/callback
  INTEGRATION_HOOK_URL: "" # Optional: URL for post-metrics integration (e.g., external analytics)
  # Complex logic comment: These can be overridden per environment; use envFrom in Deployment for injection
---
apiVersion: v1
kind: Secret
metadata:
  name: virtual-agent-secrets
  namespace: production
  labels:
    app: virtual-agent
    tier: backend
type: Opaque # Standard for generic secrets
data:
  # Base64-encoded sensitive values (example placeholders; replace with real encoded data)
  API_KEY: <BASE64_ENCODED_API_KEY> # e.g., echo -n 'your-api-key' | base64
  DATABASE_URL: <BASE64_ENCODED_DB_URL> # e.g., echo -n 'postgres://user:pass@host:5432/db' | base64
  # Complex logic comment: Secrets are mounted as env vars; use stringData in dev for plaintext, but data for prod to enforce encoding
# Note: In production, rotate secrets regularly and use RBAC to restrict access
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: virtual-agent-deployment
  namespace: production
  labels:
    app: virtual-agent
    tier: backend
  annotations:
    # Scalability enhancement: Reference for HPA (Horizontal Pod Autoscaler)
    autoscaling.alpha.kubernetes.io/hpa: "true" # Enables auto-scaling based on CPU/memory
    autoscaling/targetCPUUtilizationPercentage: "70" # Perf tweak: Scale at 70% CPU
    autoscaling/minReplicas: "2"
    autoscaling/maxReplicas: "10"
spec:
  replicas: 3 # Scalability: Start with 3 for HA; scale via HPA or manual
  selector:
    matchLabels:
      app: virtual-agent # Ensures selector matches pod labels
  strategy:
    type: RollingUpdate # Production pattern: Zero-downtime updates
    rollingUpdate:
      maxUnavailable: 1 # Limits disruptions during updates
      maxSurge: 1 # Allows one extra pod for smooth rollout
  template:
    metadata:
      labels:
        app: virtual-agent
        tier: backend
    spec:
      # Security: Run as non-root for least privilege
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000 # Arbitrary non-root UID
      # Scalability: Pod anti-affinity to spread across nodes (if topologySpreadConstraints not used)
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values: [virtual-agent]
                topologyKey: kubernetes.io/hostname # Spread across nodes for resilience
      containers:
        - name: virtual-agent
          image: virtual-agent:latest # Replace with your registry/image (e.g., gcr.io/project/virtual-agent:v1.0.0)
          ports:
            - containerPort: 8080 # Assumed agent listening port
          envFrom:
            - configMapRef:
                name: virtual-agent-config # Injects ConfigMap as env vars
            - secretRef:
                name: virtual-agent-secrets # Injects Secret as env vars (secure injection)
          # Enhanced error handling: Longer timeouts in probes for resilience
          livenessProbe:
            httpGet:
              path: /healthz # Assumed health endpoint
              port: 8080
            initialDelaySeconds: 30 # Delay for startup
            periodSeconds: 10
            timeoutSeconds: 10 # Increased for flaky networks
            failureThreshold: 3 # Allow 3 failures before restart
          readinessProbe:
            httpGet:
              path: /ready # Assumed readiness endpoint
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
            failureThreshold: 3
          resources:
            requests: # Production: Requests for scheduling guarantees (perf tweak: Reduced CPU for efficiency)
              memory: "256Mi"
              cpu: "200m" # Reduced from 250m for lighter load balancing
            limits: # Limits prevent resource starvation
              memory: "512Mi"
              cpu: "500m" # 0.5 core
          # Complex logic comment: Health probes ensure pod readiness; liveness restarts unhealthy pods, readiness controls traffic
      # Production: Node selector if agents need specific hardware (e.g., GPU for virtualization)
      # nodeSelector:
      #   node-type: agent-node
---
apiVersion: v1
kind: Service
metadata:
  name: virtual-agent-service
  namespace: production
  labels:
    app: virtual-agent
    tier: backend
spec:
  type: ClusterIP # Internal access for scalable agents; use LoadBalancer for external
  selector:
    app: virtual-agent # Matches Deployment pod labels
  ports:
    - protocol: TCP
      port: 80 # Service port (maps to pod port)
      targetPort: 8080 # Pod's listening port
  # Complex logic comment: Session affinity if agents need sticky sessions (e.g., for stateful ops); disabled for scalability
  # sessionAffinity: ClientIP
  # sessionAffinityConfig:
  #   clientIP:
  #     timeoutSeconds: 10800
